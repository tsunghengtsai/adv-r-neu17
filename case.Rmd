---
title: "Case study and typical operations with base R"
author: "Advanced R"
date: "Wednesday May 3, 2017"
output: html_document
---

```{r setup, include=FALSE}
# knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(message=FALSE, warning=FALSE)
```


## Learning goals

1. Overview of data from mass spectrometry-based proteomics experiments.

2. Basic data analysis with tools in base R.


## A typical MS-based proteomics experiment

* Design: comparison of **protein** abundance between **groups** of interest (e.g., healthy vs. diseased subjects). 

* Measurements: **peptide features** profiled using **mass spectrometry**
    - **Proteins** are cleaved into peptides.
    - **Peptides** are charged, fragmented and measured by mass spectrometry.
    - **Features** (fragments of charged peptides) are the basic unit of quantification.


```{r, warning=FALSE, message=FALSE}
load("data/adv-R-twin.RData")
```

```{r, fig.width=8, fig.height=5, echo=FALSE, fig.align='center', warning=FALSE, message=FALSE}
library(tidyverse)
library(stringr)
sub <- tbl_df(twin_dia) %>% 
    filter(str_detect(run, str_c(str_pad(1:20, 3, pad = "0"), collapse = "|"))) %>%
    rename(heavy = intensity_h, light = intensity_l) %>% 
    gather(label, intensity, heavy, light)

sub %>% filter(protein == "APOA") %>% 
    ggplot(aes(run, log2(intensity), group = feature, colour = feature)) + 
    geom_point() + 
    geom_line() + 
    ggtitle("APOA") + 
    facet_wrap(~ label) + 
    theme(axis.text.x = element_text(angle = 90, hjust = 1)) + 
    theme(legend.position = "bottom")

sub %>% 
    ggplot(aes(run, log2(intensity))) + geom_boxplot() + 
    facet_wrap(~ label) + 
    theme(axis.text.x = element_text(angle = 90, hjust = 1))
```


## Case study

Liu et al., Quantitative variability of 342 plasma proteins in a human twin population, *Molecular Systems Biology*, 2015. [PMID: 25652787]

* 232 MS runs of plasma samples:
    - 58 pairs of monozygotic (MZ) and dizygotic (DZ) twins.
    - 2 time points.
    - $58 \times 2 \times 2 = 232$.

* Data acquired with MS workflows of:
    - Data independent acquisition (DIA).
    - Selected reaction monitoring (SRM).

* Subset of the original dataset (with `r length(unique(twin_dia$protein))` proteins by DIA and `r length(unique(twin_srm$protein))` proteins by SRM).

```{r}
str(twin_dia)
```

The two data frames have the same format, containing 9 columns:

* `protein` (chr): protein name.
* `feature` (chr): combination of peptide, precursor charge state, fragment ion, and product charge state, separated by `_`.
* `run` (chr): MS run identifier (R001-R232).
* `pair` (int): pair identifier number (1-58).
* `zygosity` (factor): zygosity (MZ, DZ).
* `subject` (int): subject identifier number (1-116).
* `visit` (int): time of visit (1, 2).
* `intensity_l` (num): integrated feature intensity from light (L) channel.
* `intensity_h` (num): integrated feature intensity from heavy (H, aka reference) channel.

```{r}
head(twin_dia)
```

```{r}
class(twin_dia)
```

A **data frame** contains **variables** (in the **columns**) and **observations** (in the **rows**) in a tabular form. The columns in a data frame have the same length, but they may represent different types of variables (e.g., numeric, character, logical, etc.). Many functions for statistical modeling and inference in R presume data frames.

Dimension of the data:

```{r, eval=F}
dim(twin_dia)
nrow(twin_dia)
ncol(twin_dia)
```

Level of the categorical variable:

```{r, eval=F}
levels(twin_dia$pair)
levels(twin_dia$sample)
levels(twin_dia$label)
```

`View()` calls the data viewer in RStudio.

```{r, eval=FALSE}
View(twin_dia)
```


## Data analysis tasks for the case study

* **Task 1:** compute median of log-intensities for features from heavy channel in each run.

* **Task 2:** normalization of feature intensities to remove systematic bias across runs.

* **Task 3:** summarization of feature intensities for each protein in every run.

* **Task 4:** fit a linear model characterizing the summarized intensities.
    
* **Task 5:** group comparison using model-based inference.

We will discuss tools for wrangling data, making grouped summaries, merging data, etc. While R offers several options for each task, it is crucial to note how these approaches can (or cannot) be integrated into a consistent workflow.


## Task 1: median of log-intensities per run

We will discuss tools for data wrangling and making grouped summaries along this line.


### Data wrangling

* Reshape a dataset (without changing its content):
    - Rename the columns of a data frame. 
    - Reorder rows by values of a column. 

* Data manipulation and transformation (with `$`, indexing, logical operations):
    - Extract existing variables. 
    - Extract existing observations. 
    - Derive new variables. 


#### Rename variables

Fix typos, follow naming convention, rename columns of a data frame retuned by another function.

```{r}
colnames(twin_dia)
# colnames(twin_dia) <- c("Protein", "Feature", "Pair", "Sample", "IntensityH", "IntensityL")
```


#### Extract existing variables

To extract observations based on values in specific columns, or keep only a subset of variables relevant to the analysis.

```{r}
design <- unique(twin_dia[, c("run", "pair", "zygosity", "subject")])
head(design)
```


#### Extract existing observations with logical operations

```{r}
# Exclude rows with NA in column intensity_h
sub_dia <- twin_dia[!is.na(twin_dia$intensity_h), ]
head(sub_dia)
```

Combine multiple logical operations:

* Element-wise AND operation with `&`.
* Element-wise OR operation with `|`. 

```{r}
# Filter based on columns intensity_h and run
sub_dia <- twin_dia[!is.na(twin_dia$intensity_h) & twin_dia$run == "R001", ]
head(sub_dia)
```


#### Make new variables

```{r}
# Log2 transformation
twin_dia$log2inty_h <- log2(twin_dia$intensity_h)
twin_dia$log2inty_l <- log2(twin_dia$intensity_l)
```


### Combine multiple operatons 

Compute the median of feature intensities from heavy channel (`log2inty_h`) in one run (`R001`).

```{r}
median(twin_dia$log2inty_h[!is.na(twin_dia$log2inty_h) & twin_dia$run == "R001"])
```

* The name of the data frame is repeated several times. This can be avoided by using `with()`, as in 
`with(twin_dia, median(log2inty_h[!is.na(log2inty_h) & run == "R001"]))`.

* The combined operations are defined with a **nested representation**. The flow becomes less readable when combining several operations. Creating intermediate objects may be helpful if the objects are named properly (naming itself can be hard though).


### Split-Apply-Combine

A very common pattern to make grouped summaries: 

* **Split** a vector `X` into subsets defined by a factor vector `GROUP`.

* **Apply** function `FUN` to each subset.

* **Combine** and return the result in a convenient form.


To compute the medians of log-intensities in all runs, we need to split the column vector `log2inty_h` into subsets defined by `run`, apply `median()` to each subset, and combine the results.


### Approach 1: use a `for()` loop

```{r}
# Approach 1
runs <- unique(twin_dia$run)

medians <- rep(0, length(runs))  # create a vector to restore the result
for (i in seq_along(runs)) {
    medians[i] <- median(twin_dia$log2inty_h[twin_dia$run == runs[i]], na.rm = TRUE)
}
str(medians)
```


### Approach 2: use `tapply()`, as in `tapply(X, GROUP, FUN)`

```{r}
# Approach 2
medians <- tapply(twin_dia$log2inty_h, twin_dia$run, median, na.rm = TRUE)
head(medians)  # a named vector is returned
```


### Approach 3: use `aggregate()`, as in `aggregate(X, GROUP, FUN)`

```{r}
# Approach 3
df_median <- aggregate(twin_dia$log2inty_h, list(run = twin_dia$run), median, na.rm = TRUE)
head(df_median)  # a data frame is returned
```

```{r}
# Rename second column for later use
colnames(df_median)[2] <- "run_median"
```

`tapply()` and `aggregate()` are tools for **functional programming** that will be discussed in greater detail in a later section. Note that their outputs are of different classes.


## Task 2: normalization

Adjust the log-intensities to equalize the medians across runs to a global median: 

```{r}
(gbl_median <- median(medians, na.rm = TRUE))
```


### Approach 1: use a `for()` loop

```{r}
# Use a for loop
for (ii in names(medians)) {
    log2inty_h <- twin_dia$log2inty_h[twin_dia$run == ii]
    log2inty_l <- twin_dia$log2inty_l[twin_dia$run == ii]
    twin_dia$log2inty_h[twin_dia$run == ii] <- log2inty_h - medians[ii] + gbl_median
    twin_dia$log2inty_l[twin_dia$run == ii] <- log2inty_l - medians[ii] + gbl_median
}
```


### Approach 2: use a vectorized representation 

A more efficient way is to use a vectorized representation with the named vector returned by `tapply()`: 

```{r}
# This gives the medians of runs 001_MZ_1, 002_MZ_1, 001_MZ_1
medians[c("R001", "R002", "R003")]
```

```{r}
# Use vectorized representation to normalize the dataset
twin_dia$log2inty_h <- twin_dia$log2inty_h - medians[twin_dia$run] + gbl_median
twin_dia$log2inty_l <- twin_dia$log2inty_l - medians[twin_dia$run] + gbl_median
```


### Approach 3: use `merge()`

We can also add one additional column for the median in each run to the original dataset `twin_dia`, with `df_median` returned by `aggregate()`.

Use `merge()` to merge two data frames according to common columns, as in: `merge(x, y, by.x = "x_col", by.y = "y_col")`, to join two data frames `x`, `y`, by matching the columns `x_col` and `y_col`.

* Default (when `by.x`, `by.y` not specified) is to match all columns with common names.

* Output will be a new data frame that has all the columns of both data frames.

```{r}
twin_dia2 <- merge(x = twin_dia, y = df_median)
head(twin_dia2)

twin_dia2$log2inty_h <- twin_dia2$log2inty_h - twin_dia2$run_median + gbl_median
twin_dia2$log2inty_l <- twin_dia2$log2inty_l - twin_dia2$run_median + gbl_median
```


## Task 3: summarization of feature intensities

For each **protein** in every **run**, compute the **log of the sum** of the feature intensities.

```{r}
# Transform the normalized log-intensities back to the original scale
twin_dia$intensity_h <- 2 ^ (twin_dia$log2inty_h)
twin_dia$intensity_l <- 2 ^ (twin_dia$log2inty_l)
```


### Approach 1: use two nested `for()` loops

One for protein and the other for run. 


### Approach 2: use `tapply()`

Use `tapply()` with grouping variables defined in a **list**:

```{r}
# Approach 2
sum_t <- tapply(
    twin_dia$intensity_l, 
    list(run = twin_dia$run, protein = twin_dia$protein), 
    function(x) log2(sum(x, na.rm = TRUE))
    )
head(sum_t)
```


### Approach 3: use `aggregate()`

```{r}
# Approach 3
sum_g <- aggregate(
    twin_dia$intensity_l,
    list(run = twin_dia$run, protein = twin_dia$protein),
    function(x) log2(sum(x, na.rm = TRUE))
)
head(sum_g)
```

```{r, eval=FALSE}
# Alternatively, use a formula representation
aggregate(
    intensity_l ~ run + protein, 
    data = twin_dia, 
    function(x) log2(sum(x, na.rm = TRUE))
)
```

```{r}
# Rename the third column
colnames(sum_g)[3] <- "log2inty"
```

Saving grouped summaries in a data frame makes it easier to carry out statistical modeling and inference in subsequent analysis.


## Task 4: statistical modeling

For each **protein**, fit a linear model, extract summaries of the fitted model, and draw model-based conclusions.


### Fit a linear regression model with `lm()`

The first argument of `lm()` is a formula, e.g., `Y ~ X1 + X2`, where `Y` is the response and `X1`, `X2` are the predictive variables. These refer to the column names (variables) in a data frame, that we pass in `lm()` through the `data` argument.

Suppose we are interested in knowing if the abundance of one **protein** is associated with **zygosity**.

```{r}
# Merge the design information
df_sum <- merge(sum_g, design)
head(df_sum)

sub_sum <- df_sum[df_sum$protein == "A2MG", ]  # Subset for protein A2MG
fit <- lm(log2inty ~ zygosity, data = sub_sum)
```

```{r, eval=FALSE}
# Same as
fit <- lm(log2inty ~ zygosity, data = df_sum, subset = df_sum$protein == "A2MG")
```

```{r}
class(fit)
```


### Utility functions

R offers a couple of utility functions for linear models, such as `coef()`, `fitted()`, `residuals()`, `summary()`, `predict()`, for retrieving coefficients, fitted values, residuals, model summary, making predictions with the model, respectively.

These functions can be applied in the same way for model objects from `glm()`, `gam()`, etc.


#### Use `summary()` to display an overview of the fitted model

```{r}
summary(fit)
```

```{r, eval=FALSE}
# More detail
str(summary(fit))
```


#### Use `coef()` to retrieve estimated coefficients

```{r}
coef(fit)
```


#### Use `fitted()` to retrieve fitted values

```{r}
head(fitted(fit))
```

It is not very easy to develop general workflow deriving summaries for all the proteins. Without defining specialized functions to extract information on various aspects (which can be called by `tapply()` or `aggregate()`), we are mostly left off with iterations due to

* more complicated/involved operations
* complex output objects. 


## Task 5: model-based inference

Two-sample $t$-test

```{r}
ttest <- t.test(log2inty ~ zygosity, data = sub_sum)
ttest
```

Alternative, use the `subset` option:

```{r, eval=FALSE}
t.test(log2inty ~ zygosity, data = df_sum, subset = df_sum$protein == "A2MG")
```

Use two vectors for the two samples:

```{r, eval=FALSE}
t.test(x = sub_sum$log2inty[sub_sum$zygosity == "DZ"], y = sub_sum$log2inty[sub_sum$zygosity == "MZ"])
```

The output object is again quite complex.

```{r}
str(ttest)
```

```{r}
ttest$estimate
ttest$statistic
ttest$p.value
```

It is therefore not very easy to generalize. How would you implement a workflow to summarize the means of two groups, difference, $t$-statistic, $p$-value in every protein? 

